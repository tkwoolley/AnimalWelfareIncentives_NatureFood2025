{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d651d79f-5467-47b9-b6d6-3a9323732e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\trevor_woolley\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  # Import the stopwords module from NLTK\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load data from Excel\n",
    "df = pd.read_excel('LDA_Abstracts.xls')\n",
    "\n",
    "# Remove stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Add custom stopwords\n",
    "custom_stopwords = [\"50\", \"95\", \"reserved\", \"It\", \"05\", \"study\", \"conducted\", \"xhtml\", \"w3\", \"sup\", \"sub\", \"xmlns\", \"1999\", \"elsevier\", \"ltd\",  \"org\", \"www\", \"significant\", \"significantly\", \"higher\", \"http\", \"mg\", \"kg\"]\n",
    "\n",
    "# Extend the default list with custom stopwords\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "# Define a function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if text is a string\n",
    "        # Tokenize text and remove stopwords and punctuation\n",
    "        tokens = re.findall(r'\\b\\w+\\b', text.lower())  # Tokenize text\n",
    "        tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "        return ' '.join(tokens)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# Apply preprocess_text function\n",
    "df['Content'] = df['abstract'].apply(preprocess_text)\n",
    "\n",
    "# Vectorize content\n",
    "vect = CountVectorizer(max_df=0.95, min_df=10, ngram_range=(2, 3))  # Limit phrases to 2 to 3 words\n",
    "dtm = vect.fit_transform(df['Content'])\n",
    "\n",
    "# (change n_components) Apply LDA to get 30 topics (random state is the seed)\n",
    "lda = LatentDirichletAllocation(n_components=30, random_state=42, doc_topic_prior=0.0000001, topic_word_prior=0.0000001)\n",
    "lda_topics = lda.fit_transform(dtm)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vect.get_feature_names()\n",
    "\n",
    "# Get topic words (phrases)\n",
    "topic_phrases = {}\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    topic_indices = topic.argsort()[-10:] # This -10 has nothing to do with n_components\n",
    "    topic_words = [feature_names[index] for index in topic_indices]\n",
    "    topic_phrases[i] = ' '.join(topic_words)\n",
    "\n",
    "# (change range) Add topics as columns \n",
    "for i in range(30):\n",
    "    col_name = 'Topic_' + str(i)        \n",
    "    df[col_name] = lda_topics[:, i]\n",
    "\n",
    "# (change range) Calculate propensity scores    \n",
    "for i in range(30):\n",
    "    col_name = 'Topic_' + str(i)\n",
    "    \n",
    "    df[col_name + '_Score'] = df.apply(lambda x: \n",
    "        (sum([1 for word in x['Content'].split() if word in topic_phrases[i].split()]) / len(x['Content'].split())) if len(x['Content'].split()) > 0 else 0, \n",
    "        axis=1)\n",
    "\n",
    "# Export DataFrame with phrases and corresponding topic numbers\n",
    "phrases_df = pd.DataFrame({'Topic Phrases': topic_phrases.values(), 'Topic Number': topic_phrases.keys()})\n",
    "phrases_df.to_csv('lda_topic_phrases.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# (change range) Specify the columns to save in the 'lda_output.csv', including the \"id\" column\n",
    "columns_to_save = ['id', 'abstract'] + [f'Topic_{i}' for i in range(30)] + [f'Topic_{i}_Score' for i in range(30)]\n",
    "df.to_csv('lda_output.csv', index=False, encoding='utf-8', columns=columns_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59815438-47d9-4c37-ba00-e84706e0e1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
